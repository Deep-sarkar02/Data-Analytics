# -*- coding: utf-8 -*-
"""Session -27 MULTI INDEXING CONTINUED AND MELT ,PIVOT ,JOINS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/123mc2dP88_VNjCqFu5BXw9qfjCR2dfEh
"""

import pandas as pd

# Define MultiIndex for rows: Year and Product Category
index = pd.MultiIndex.from_product(
    [[2019, 2020, 2021, 2022, 2023, 2024], ['electric', 'cloting', 'machinary']],
    names=['Year', '']
)

# Define MultiIndex for columns: City and Metric
columns = pd.MultiIndex.from_product(
    [['Delhi', 'Mumbai'], ['Sales', 'Quantity']],
    names=['City', 'Metric']
)

# Data rows in order as per the image
data = [
    [2131, 82, 4945, 14], [2979, 34, 9520, 39], [2334, 72, 4797, 82],
    [9913, 15, 8750, 44], [2377, 20, 1710, 77], [5523, 53, 8725, 98],
    [6984, 94, 5410, 28], [4752, 12, 4584, 52], [1113, 22, 9442, 10],
    [3625, 57, 8123, 46], [6831, 44, 7264, 83], [2580, 24, 3827, 39],
    [7931, 85, 7202, 20], [5425, 91, 5676, 71], [3204, 73, 8815, 31],
    [1906, 21, 7860, 99], [9935, 69, 9912, 89], [5734, 35, 8555, 42]
]

# Create the DataFrame
df = pd.DataFrame(data, index=index, columns=columns)
print(df)

df

"""#sorting on basis of indedx

"""

df.sort_index() # sorting in the ascending order

# for descending order
df.sort_index(ascending = False)

# now if l1 - ascending
# l2 = descending
df.sort_index(ascending = [True , False])

#  if i want to descending the level0 only and level 1 will be as it is
df.sort_index(level = 1 , ascending = False)

# for the colm sorting
df.columns

# for colms
df.sort_values(by = ( 'Delhi',    'Sales'), ascending = False)

"""#swap - levels"""

df.swaplevel() #for row level swap

# for colm level swap
df.swaplevel(axis = 1)

"""#droplevel"""

#we can drop any of the level from the levels
df.droplevel(1 , axis = 0)

"""#long and wide format of the data"""

#take a data
df1 = pd.DataFrame({
    'Student': ['Alice', 'Bob', 'Charlie'],
    'Math': [85, 90, 78],
    'Science': [92, 88, 80],
    'English': [88, 76, 85]
})

"""#wide to long format of data"""

df1 #--wide form of data

"""#use of melt function

"""

df1.melt() #== converted to long format
# but the colm headers are the row in here which is not very redable

#usr of id_vars
df1.melt(id_vars = "Student")

#chnage the name of the colm
#usr of id_vars
df1.melt(id_vars = "Student" , var_name = "subject" , value_name = "Marks")

"""#create another df"""

df2 = pd.DataFrame({
    'Student': ['Alice', 'Bob', 'Charlie'],
    'Semester': ['First', 'Second', 'Third'],
    'Math': [85, 90, 78],
    'Science': [92, 88, 80],
    'English': [88, 76, 85]
})

df2

# now if i dont to melt the first 2 rows for that

# now we will pass alist where we can mention
df2.melt(id_vars = ["Student" , "Semester"])

df2

# now we can add which colm only to tbe melted
df2.melt(value_vars = "Semester")

"""#pivot table :- way to summerise the data by grouping and performing op like sum , mean and other aggregate function"""

sp = pd.read_csv('/content/spending_patterns_detailed.csv')

sp

# covert the transactiion date to datetime
sp['Transaction Date'] = pd.to_datetime(sp['Transaction Date'])

sp.sample(3)

# now find the total spend per category
sp.groupby("Category")['Total Spent'].sum()

# now find the same for the total spent and payment method
sp.groupby(['Category', 'Payment Method'])['Total Spent'].sum().unstack()

# this an be achieved by the help of pivit table
# by the easy way
# main 3 things:
#index ,cols, value ,
sp.pivot_table(index = 'Category' , columns = 'Payment Method' ,values = 'Total Spent', aggfunc = "sum")

# now we will take the category and th paymennt method as the index
#all the colm willbe year and the month
# and the values will be total spent

sp.pivot_table(
    index = ['Category' ,'Payment Method'	],
    columns = [sp['Transaction Date'].dt.year.rename("year"),
               sp['Transaction Date'].dt.month.rename("Month")],
    values = 'Total Spent',
    aggfunc = "std"
)

# now we will tke a data from seaborn
import seaborn as sns
df3 = sns.load_dataset("tips")

df3.sample(4)

# convert it to pivot
df3.pivot_table(index = ['sex' , 'smoker'] ,
                columns = ['day' , 'time'],
                aggfunc ={'size' : 'min',
                          'total_bill' : "max"} )

# another function
df3.pivot_table(index = "smoker" ,
                columns = "sex",
                values = "total_bill",
                aggfunc = "sum" ,
                margins = True).reset_index()

# take the spending dsta
sp

# plot the graph  for total bill month wise and cat wise
# ie for each of the montth , each categroy r kotot bill hoyeche


pivot =sp.pivot_table(index = 'Category' ,
               columns = sp['Transaction Date'].dt.to_period('M'),
               values = 'Total Spent')

pivot

# but we will plot this so the x axis must be the month so we ill make this transpose
pivot.T

import matplotlib.pyplot as plt
# now plot the data
pivot.T.plot(kind = "line" ,marker = "o" , figsize=(20 ,10) )

# title
plt.title("total spending over time per category")

plt.xlabel("month")
plt.ylabel("total spent")

"""#Questions"""

df4 = pd.DataFrame({
    'Store_ID': [101, 102, 103, 104],
    'Location': ['New York', 'New York', 'Chicago', 'Chicago'],
    'Manager': ['Alice', 'Bob', 'Charlie', 'David'],
    'Q1_Sales': [25000, 18000, 22000, 20000],
    'Q2_Sales': [27000, 19000, 24000, 21000],
    'Q1_Profit': [5000, 3000, 4500, 4000],
    'Q2_Profit': [6000, 3200, 4800, 4200],
    'Q1_Cust_Satisfaction': [4.5, 4.2, 4.0, 4.1],
    'Q2_Cust_Satisfaction': [4.6, 4.3, 4.1, 4.2]
})

df4

"""#find location wise , avg sales and profit for each quater"""

df4.melt(id_vars="Location", value_vars=["Q1_Sales", "Q2_Sales", "Q1_Profit", "Q2_Profit"])

df4.melt(id_vars="Location", value_vars=["Q1_Sales", "Q2_Sales", "Q1_Profit", "Q2_Profit"]).groupby(["Location","variable"])["value"].mean().unstack()

# chek the cust satisfacrtion on basid of the location and the quaters
df4 # eow again make this in the another foramt

# so we will melet the table for the q1 and q2 cust satisfaction
# and we will not melt the location
df4.melt(id_vars = 'Location' , value_vars = ['Q1_Cust_Satisfaction','Q2_Cust_Satisfaction'])

# now groupby
df4.melt(id_vars = 'Location' , value_vars = ['Q1_Cust_Satisfaction','Q2_Cust_Satisfaction']).groupby(["Location","variable"])['value'].mean().unstack()

df4

# find the % chage in the sales,profit from q1 to q2 for each location
# for eahc location , we will find total sales

# we will convert the col
df4

# so we can groupby but we need to convert it to rows
df_long = df4.melt(id_vars=['Location'],# since we want the colm location
         value_vars = ['Q1_Sales',	'Q2_Sales' ,	'Q1_Profit',	'Q2_Profit'],
         var_name = "metric",
         value_name = "value")

df_long

# now from this we need sales and profit 2 difff colm
# so we will split Q1_Sales into quater and sales
# or profit
df_long[['Quater' , "metric"]]=df_long['metric'].str.split("_" ,expand = True )

df_long

# now we can form the pivot
df_piv = df_long.pivot_table(
    index = ['Location' , 'metric'] ,
    columns = 'Quater',
    values = 'value'
)

df_piv

# now we will do the % chnage
df_piv['% change'] = ((df_piv['Q2']-df_piv['Q1'])/df_piv['Q1'])*100

df_piv

"""#q2"""

df5 = pd.DataFrame({
    "Product": ["Laptop", "Phone", "Tablet", "Monitor", "Headphone"],
    "Jan_Sales": [120, 200, 90, 50, 70],
    "Feb_Sales": [150, 220, 100, 60, 80],
    "Mar_Sales": [130, 210, 110, 65, 85],
    "Apr_Sales": [170, 250, 120, 70, 95]
})

df5

# fidn the highest selling prod for each month

# convert to proper structure
# dont melt the product
df_long = df5.melt(id_vars = ['Product'],var_name = "Month", value_name = "sales")

df_long

# now we will replace the _sales with space
df_long['Month']=df_long['Month'].str.replace("_Sales" , "")

df_long

# find the highest selling product
df_long.loc[df_long.groupby("Month")['sales'].idxmax()]

"""#concatination"""

# create a df
import numpy as np
df1 = pd.DataFrame({
    "Product_id": np.random.randint(1, 10, 10),
    "Quantity": np.random.randint(1, 10, 10),
    "Profit": np.random.randint(1000, 5000, 10)
})

# Second DataFrame
df2 = pd.DataFrame({
    "Product_id": np.random.randint(1, 10, 10),
    "Quantity": np.random.randint(1, 10, 10),
    "Profit": np.random.randint(1000, 5000, 10)
})

df1

df2

df_c = pd.concat([df1 , df2])

# now if we want to access the 0 th row
df_c.loc[0]

df_d = pd.concat([df1 , df2] , ignore_index = True)

df_d

# we can also use the keys param
df_e = pd.concat([df1 , df2] , keys = ['df1' , 'df2'])

df_e

